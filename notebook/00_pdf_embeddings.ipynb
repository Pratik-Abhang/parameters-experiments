{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# 00 - PDF Embeddings & RAG Pipeline Setup\n",
    "\n",
    "This notebook processes the Gemini PDF and creates the foundation for all RAG experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # pymupdf\n",
    "from openai import OpenAI\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import time\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF loaded: 222829 characters\n",
      "First 500 chars: Gemini: A Family of Highly Capable\n",
      "Multimodal Models\n",
      "Gemini Team, Google1\n",
      "This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities\n",
      "across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano\n",
      "sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained\n",
      "use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model\n",
      "advances the state...\n"
     ]
    }
   ],
   "source": [
    "# Load PDF\n",
    "PDF_PATH = \"../data/Gemini_FamilyOfMultimodelModels.pdf\"\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        pdf_reader = fitz.open(file)\n",
    "        for page in pdf_reader:\n",
    "            text += page.get_text() + \"\\n\"\n",
    "    return text\n",
    "\n",
    "pdf_text = extract_text_from_pdf(PDF_PATH)\n",
    "print(f\"PDF loaded: {len(pdf_text)} characters\")\n",
    "print(f\"First 500 chars: {pdf_text[:500]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 82 chunks\n",
      "Sample chunk: Gemini: A Family of Highly Capable Multimodal Models Gemini Team, Google1 This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, vi...\n"
     ]
    }
   ],
   "source": [
    "# Text chunking function\n",
    "def chunk_text(text, chunk_size=500, overlap=100):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    \n",
    "    step = chunk_size - overlap\n",
    "    \n",
    "    for i in range(0, len(words), step):\n",
    "        chunk = \" \".join(words[i:i+chunk_size])\n",
    "        if len(chunk.strip()) > 50:  # Skip very small chunks\n",
    "            chunks.append({\n",
    "                'text': chunk,\n",
    "                'chunk_id': len(chunks),\n",
    "                'word_count': len(chunk.split())\n",
    "            })\n",
    "        \n",
    "        if i + chunk_size >= len(words):\n",
    "            break\n",
    "            \n",
    "    return chunks\n",
    "\n",
    "# Create chunks with default parameters\n",
    "chunks = chunk_text(pdf_text, chunk_size=500, overlap=100)\n",
    "print(f\"Created {len(chunks)} chunks\")\n",
    "print(f\"Sample chunk: {chunks[0]['text'][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings...\n",
      "Processing batch 1/9\n",
      "Processing batch 2/9\n",
      "Processing batch 3/9\n",
      "Processing batch 4/9\n",
      "Processing batch 5/9\n",
      "Processing batch 6/9\n",
      "Processing batch 7/9\n",
      "Processing batch 8/9\n",
      "Processing batch 9/9\n",
      "Generated 82 embeddings\n"
     ]
    }
   ],
   "source": [
    "# Generate embeddings\n",
    "def get_embedding(text, model=\"text-embedding-3-small\"):\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    return client.embeddings.create(input=[text], model=model).data[0].embedding\n",
    "\n",
    "def generate_embeddings(chunks, batch_size=10):\n",
    "    embeddings = []\n",
    "    \n",
    "    for i in range(0, len(chunks), batch_size):\n",
    "        batch = chunks[i:i+batch_size]\n",
    "        print(f\"Processing batch {i//batch_size + 1}/{(len(chunks)-1)//batch_size + 1}\")\n",
    "        \n",
    "        for chunk in batch:\n",
    "            embedding = get_embedding(chunk['text'])\n",
    "            embeddings.append(embedding)\n",
    "            \n",
    "        time.sleep(0.1)  # Rate limiting\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "# Generate embeddings (this will take a few minutes)\n",
    "print(\"Generating embeddings...\")\n",
    "embeddings = generate_embeddings(chunks)\n",
    "print(f\"Generated {len(embeddings)} embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG data saved to ../data/rag_embeddings.pkl\n"
     ]
    }
   ],
   "source": [
    "# Save embeddings and chunks\n",
    "rag_data = {\n",
    "    'chunks': chunks,\n",
    "    'embeddings': embeddings,\n",
    "    'metadata': {\n",
    "        'pdf_path': PDF_PATH,\n",
    "        'chunk_size': 500,\n",
    "        'overlap': 100,\n",
    "        'total_chunks': len(chunks),\n",
    "        'embedding_model': 'text-embedding-3-small'\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('../data/rag_embeddings.pkl', 'wb') as f:\n",
    "    pickle.dump(rag_data, f)\n",
    "\n",
    "print(\"RAG data saved to ../data/rag_embeddings.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What are the key capabilities of Gemini models?\n",
      "\n",
      "Top 3 retrieved chunks:\n",
      "\n",
      "Rank 1 (similarity: 0.731):\n",
      "testing on Gemini Advanced: â€¢ Priority User Program: This program collected feedback from 120 power users, key influencers, and thought-leaders. This program enables the collection of real-time feedba...\n",
      "\n",
      "Rank 2 (similarity: 0.719):\n",
      "Ruddock, Art Khurshudov, Artemis Chen, Arthur Argenson, Avinatan Hassidim, Beiye Liu, Benjamin Schroeder, Bin Ni, Brett Daw, Bryan Chiang, Burak Gokturk, Carl Crous, Carrie Grimes Bostock, Charbel Kae...\n",
      "\n",
      "Rank 3 (similarity: 0.703):\n",
      "of high-quality demonstration data and feedback data for coding use cases. Gemini Apps and Gemini API models use a combination of human and synthetic approaches to collect such data. We evaluate our G...\n"
     ]
    }
   ],
   "source": [
    "# RAG retrieval function\n",
    "def retrieve_chunks(query, chunks, embeddings, k=5):\n",
    "    query_embedding = get_embedding(query)\n",
    "    \n",
    "    # Calculate similarities\n",
    "    similarities = cosine_similarity([query_embedding], embeddings)[0]\n",
    "    \n",
    "    # Get top k chunks\n",
    "    top_indices = np.argsort(similarities)[::-1][:k]\n",
    "    \n",
    "    retrieved_chunks = []\n",
    "    for idx in top_indices:\n",
    "        retrieved_chunks.append({\n",
    "            'chunk': chunks[idx],\n",
    "            'similarity': similarities[idx],\n",
    "            'rank': len(retrieved_chunks) + 1\n",
    "        })\n",
    "    \n",
    "    return retrieved_chunks\n",
    "\n",
    "# Test retrieval\n",
    "test_query = \"What are the key capabilities of Gemini models?\"\n",
    "retrieved = retrieve_chunks(test_query, chunks, embeddings, k=3)\n",
    "\n",
    "print(f\"Query: {test_query}\")\n",
    "print(\"\\nTop 3 retrieved chunks:\")\n",
    "for item in retrieved:\n",
    "    print(f\"\\nRank {item['rank']} (similarity: {item['similarity']:.3f}):\")\n",
    "    print(f\"{item['chunk']['text'][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What are the main capabilities of Gemini models?\n",
      "\n",
      "Answer: Gemini models exhibit a range of advanced capabilities across multiple modalities, including text, code, image, audio, and video. Specifically, the most capable model, Gemini Ultra, demonstrates significant performance improvements in several areas:\n",
      "\n",
      "1. **Natural Language Processing**: Gemini Ultra surpasses human-expert performance on the MMLU exam benchmark, scoring 90.0%, indicating its advanced capabilities in understanding and generating human language.\n",
      "\n",
      "2. **Multimodal Understanding**: The models excel in image understanding, video understanding, and audio understanding benchmarks without requiring task-specific modifications or tuning. This includes the ability to parse complex images, such as charts and infographics, and reason over interleaved sequences of images, audio, and text.\n",
      "\n",
      "3. **Reasoning Capabilities**: Gemini models are noted for their reasoning abilities, which enable them to tackle complex multi-step problems. This is particularly highlighted in the development of AlphaCode 2, a Gemini-model-powered agent that excels in competitive programming, ranking within\n",
      "\n",
      "Metrics:\n",
      "- Latency: 4.80s\n",
      "- Context length: 10345 chars\n",
      "- Tokens used: 2538\n"
     ]
    }
   ],
   "source": [
    "# RAG generation function\n",
    "def rag_generate(query, chunks, embeddings, retrieval_k=5, **llm_params):\n",
    "    # Retrieve relevant chunks\n",
    "    retrieved = retrieve_chunks(query, chunks, embeddings, k=retrieval_k)\n",
    "    \n",
    "    # Build context\n",
    "    context = \"\\n\\n\".join([item['chunk']['text'] for item in retrieved])\n",
    "    \n",
    "    # Generate response\n",
    "    prompt = f\"\"\"\n",
    "Use the context below to answer the question. Be accurate and cite specific information from the context.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=llm_params.get('temperature', 0.3),\n",
    "        max_tokens=llm_params.get('max_tokens', 300),\n",
    "        top_p=llm_params.get('top_p', 0.9)\n",
    "    )\n",
    "    \n",
    "    latency = time.time() - start_time\n",
    "    \n",
    "    return {\n",
    "        'query': query,\n",
    "        'answer': response.choices[0].message.content,\n",
    "        'retrieved_chunks': retrieved,\n",
    "        'context_length': len(context),\n",
    "        'latency': latency,\n",
    "        'usage': response.usage,\n",
    "        'parameters': llm_params\n",
    "    }\n",
    "\n",
    "# Test RAG pipeline\n",
    "result = rag_generate(\n",
    "    \"What are the main capabilities of Gemini models?\",\n",
    "    chunks, embeddings,\n",
    "    retrieval_k=3,\n",
    "    temperature=0.3,\n",
    "    max_tokens=200\n",
    ")\n",
    "\n",
    "print(f\"Query: {result['query']}\")\n",
    "print(f\"\\nAnswer: {result['answer']}\")\n",
    "print(f\"\\nMetrics:\")\n",
    "print(f\"- Latency: {result['latency']:.2f}s\")\n",
    "print(f\"- Context length: {result['context_length']} chars\")\n",
    "print(f\"- Tokens used: {result['usage'].total_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test queries for parameter experiments:\n",
      "1. What are the key capabilities of Gemini models?\n",
      "2. How does Gemini compare to other multimodal models?\n",
      "3. What are the different versions of Gemini?\n",
      "4. What training data was used for Gemini?\n",
      "5. What are the safety measures in Gemini models?\n",
      "\n",
      "RAG pipeline ready! Use these queries in notebooks 01-09 for parameter testing.\n"
     ]
    }
   ],
   "source": [
    "# Test queries for experiments\n",
    "TEST_QUERIES = [\n",
    "    \"What are the key capabilities of Gemini models?\",\n",
    "    \"How does Gemini compare to other multimodal models?\",\n",
    "    \"What are the different versions of Gemini?\",\n",
    "    \"What training data was used for Gemini?\",\n",
    "    \"What are the safety measures in Gemini models?\"\n",
    "]\n",
    "\n",
    "print(\"Test queries for parameter experiments:\")\n",
    "for i, query in enumerate(TEST_QUERIES, 1):\n",
    "    print(f\"{i}. {query}\")\n",
    "\n",
    "print(f\"\\nRAG pipeline ready! Use these queries in notebooks 01-09 for parameter testing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabf6d76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
