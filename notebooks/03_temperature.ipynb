{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# 03 - Temperature Parameter Comparison\n",
    "\n",
    "This notebook compares different temperature values for RAG applications:\n",
    "- Temperature: 0.0, 0.2, 0.5, 0.8, 1.2, 1.5, 2.0\n",
    "\n",
    "We'll evaluate: accuracy, creativity, diversity, consistency, and response quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "load_dotenv()\n",
    "client = OpenAI()\n",
    "\n",
    "# Test queries about Gemini\n",
    "TEST_QUERIES = [\n",
    "    \"What are the key capabilities of Gemini models?\",\n",
    "    \"How does Gemini compare to other multimodal models?\", \n",
    "    \"What are the different versions of Gemini?\",\n",
    "    \"What training data was used for Gemini?\",\n",
    "    \"What are the safety measures in Gemini models?\",\n",
    "    \"How does Gemini perform on benchmarks?\",\n",
    "    \"What is the architecture of Gemini?\",\n",
    "    \"What are the limitations of Gemini?\"\n",
    "]\n",
    "\n",
    "# Temperature values to test\n",
    "TEMPERATURES = [0.0, 0.2, 0.5, 0.8, 1.2, 1.5, 2.0]\n",
    "\n",
    "print(f\"Testing {len(TEMPERATURES)} temperature values\")\n",
    "print(f\"Using {len(TEST_QUERIES)} test queries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced LLM Judge for Temperature-Specific Evaluation\n",
    "def llm_judge_temperature(query, answer, context, judge_model=\"gpt-4o-mini\"):\n",
    "    judge_prompt = f\"\"\"\n",
    "You are an expert evaluator assessing RAG responses with focus on temperature effects.\n",
    "\n",
    "CONTEXT:\n",
    "{context[:2000]}...\n",
    "\n",
    "QUESTION: {query}\n",
    "\n",
    "ANSWER: {answer}\n",
    "\n",
    "Evaluate on these 6 criteria using this scale:\n",
    "- Poor (1): Severely lacking\n",
    "- Below Average (2): Partially adequate\n",
    "- Average (3): Meets basic requirements\n",
    "- Good (4): High quality\n",
    "- Excellent (5): Outstanding\n",
    "\n",
    "CRITERIA:\n",
    "1. Accuracy: Factual correctness based on context\n",
    "2. Completeness: Addresses all parts of the question\n",
    "3. Clarity: Well-structured and understandable\n",
    "4. Creativity: Novel insights or creative explanations\n",
    "5. Diversity: Varied vocabulary and expression\n",
    "6. Consistency: Logical flow and coherence\n",
    "\n",
    "Respond in this exact format:\n",
    "Accuracy: [Rating]\n",
    "Completeness: [Rating]\n",
    "Clarity: [Rating]\n",
    "Creativity: [Rating]\n",
    "Diversity: [Rating]\n",
    "Consistency: [Rating]\n",
    "Overall: [Rating]\n",
    "Reasoning: [Brief explanation]\n",
    "\"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=judge_model,\n",
    "            messages=[{\"role\": \"user\", \"content\": judge_prompt}],\n",
    "            temperature=0.1,\n",
    "            max_tokens=400\n",
    "        )\n",
    "        \n",
    "        evaluation = response.choices[0].message.content\n",
    "        lines = evaluation.strip().split('\\n')\n",
    "        result = {}\n",
    "        \n",
    "        for line in lines:\n",
    "            if ':' in line:\n",
    "                key, value = line.split(':', 1)\n",
    "                result[key.strip().lower()] = value.strip()\n",
    "        \n",
    "        return {\n",
    "            'accuracy': result.get('accuracy', 'N/A'),\n",
    "            'completeness': result.get('completeness', 'N/A'),\n",
    "            'clarity': result.get('clarity', 'N/A'),\n",
    "            'creativity': result.get('creativity', 'N/A'),\n",
    "            'diversity': result.get('diversity', 'N/A'),\n",
    "            'consistency': result.get('consistency', 'N/A'),\n",
    "            'overall': result.get('overall', 'N/A'),\n",
    "            'reasoning': result.get('reasoning', 'N/A')\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'accuracy': 'Error',\n",
    "            'completeness': 'Error',\n",
    "            'clarity': 'Error',\n",
    "            'creativity': 'Error',\n",
    "            'diversity': 'Error',\n",
    "            'consistency': 'Error',\n",
    "            'overall': 'Error',\n",
    "            'reasoning': f'Evaluation failed: {str(e)}'\n",
    "        }\n",
    "\n",
    "print(\"Enhanced LLM Judge function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load RAG embeddings and setup\n",
    "with open('../data/rag_embeddings.pkl', 'rb') as f:\n",
    "    rag_data = pickle.load(f)\n",
    "\n",
    "chunks = rag_data['chunks']\n",
    "embedding_results = rag_data['embedding_results']\n",
    "\n",
    "# Use the best performing embedding model (text-embedding-3-small as default)\n",
    "embeddings = embedding_results['text-embedding-3-small']['embeddings']\n",
    "\n",
    "print(f\"Loaded {len(chunks)} chunks for RAG experiments\")\n",
    "print(f\"Using embeddings from text-embedding-3-small model\")\n",
    "\n",
    "# RAG retrieval function\n",
    "def get_embedding(text, model=\"text-embedding-3-small\"):\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    return client.embeddings.create(input=[text], model=model).data[0].embedding\n",
    "\n",
    "def retrieve_chunks(query, chunks, embeddings, k=3):\n",
    "    query_embedding = get_embedding(query)\n",
    "    similarities = cosine_similarity([query_embedding], embeddings)[0]\n",
    "    top_indices = np.argsort(similarities)[::-1][:k]\n",
    "    \n",
    "    retrieved_chunks = []\n",
    "    for idx in top_indices:\n",
    "        retrieved_chunks.append({\n",
    "            'chunk': chunks[idx],\n",
    "            'similarity': similarities[idx]\n",
    "        })\n",
    "    \n",
    "    return retrieved_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temperature RAG Experiment Function\n",
    "def run_temperature_rag_experiment(query, temp):\n",
    "    start = time.time()\n",
    "    \n",
    "    # Retrieve relevant chunks\n",
    "    retrieved = retrieve_chunks(query, chunks, embeddings, k=3)\n",
    "    context = \"\\n\\n\".join([item['chunk']['text'] for item in retrieved])\n",
    "    \n",
    "    # Generate response with context\n",
    "    prompt = f\"\"\"\n",
    "Use the context below to answer the question. Be accurate and cite specific information from the context.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=temp,\n",
    "        max_tokens=200\n",
    "    )\n",
    "    \n",
    "    latency = time.time() - start\n",
    "    answer = response.choices[0].message.content\n",
    "    \n",
    "    # Get enhanced LLM judge evaluation\n",
    "    evaluation = llm_judge_temperature(query, answer, context)\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    word_count = len(answer.split())\n",
    "    unique_words = len(set(answer.lower().split()))\n",
    "    lexical_diversity = unique_words / word_count if word_count > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"temperature\": temp,\n",
    "        \"answer\": answer,\n",
    "        \"context_length\": len(context),\n",
    "        \"latency\": round(latency, 2),\n",
    "        \"tokens\": response.usage.total_tokens,\n",
    "        \"word_count\": word_count,\n",
    "        \"unique_words\": unique_words,\n",
    "        \"lexical_diversity\": round(lexical_diversity, 3),\n",
    "        \"accuracy\": evaluation['accuracy'],\n",
    "        \"completeness\": evaluation['completeness'],\n",
    "        \"clarity\": evaluation['clarity'],\n",
    "        \"creativity\": evaluation['creativity'],\n",
    "        \"diversity\": evaluation['diversity'],\n",
    "        \"consistency\": evaluation['consistency'],\n",
    "        \"overall_rating\": evaluation['overall'],\n",
    "        \"reasoning\": evaluation['reasoning']\n",
    "    }\n",
    "\n",
    "print(\"Temperature RAG experiment function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Temperature RAG Experiments\n",
    "print(\"Running Temperature RAG experiments...\")\n",
    "results = []\n",
    "\n",
    "for query in TEST_QUERIES:\n",
    "    for temp in TEMPERATURES:\n",
    "        print(f\"Testing query: '{query[:50]}...' at temperature {temp}\")\n",
    "        result = run_temperature_rag_experiment(query, temp)\n",
    "        results.append(result)\n",
    "        time.sleep(1)  # Rate limiting\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "print(f\"\\nCompleted {len(results)} Temperature RAG experiments\")\n",
    "print(f\"Total combinations: {len(TEST_QUERIES)} queries √ó {len(TEMPERATURES)} temperatures = {len(TEST_QUERIES) * len(TEMPERATURES)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results summary\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "df.to_csv('../data/temperature_rag_results.csv', index=False)\n",
    "print(\"Results saved to temperature_rag_results.csv\")\n",
    "\n",
    "# Save detailed results with pickle\n",
    "with open('../data/temperature_rag_detailed.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'results': results,\n",
    "        'test_queries': TEST_QUERIES,\n",
    "        'temperatures': TEMPERATURES,\n",
    "        'chunks': chunks,\n",
    "        'embeddings': embeddings\n",
    "    }, f)\n",
    "print(\"Detailed results saved to temperature_rag_detailed.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert ratings to numeric for analysis\n",
    "rating_map = {'Poor': 1, 'Below Average': 2, 'Average': 3, 'Good': 4, 'Excellent': 5}\n",
    "\n",
    "for col in ['accuracy', 'completeness', 'clarity', 'creativity', 'diversity', 'consistency', 'overall_rating']:\n",
    "    df[f'{col}_numeric'] = df[col].map(rating_map)\n",
    "\n",
    "print(\"Converted ratings to numeric values for analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical Summary by Temperature\n",
    "summary_stats = df.groupby('temperature').agg({\n",
    "    'accuracy_numeric': ['mean', 'std'],\n",
    "    'completeness_numeric': ['mean', 'std'],\n",
    "    'clarity_numeric': ['mean', 'std'],\n",
    "    'creativity_numeric': ['mean', 'std'],\n",
    "    'diversity_numeric': ['mean', 'std'],\n",
    "    'consistency_numeric': ['mean', 'std'],\n",
    "    'overall_rating_numeric': ['mean', 'std'],\n",
    "    'latency': ['mean', 'std'],\n",
    "    'tokens': ['mean', 'std'],\n",
    "    'lexical_diversity': ['mean', 'std']\n",
    "}).round(3)\n",
    "\n",
    "print(\"Temperature Performance Summary:\")\n",
    "print(summary_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualizations\n",
    "plt.style.use('default')\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Temperature Effects on RAG Performance', fontsize=16, fontweight='bold')\n",
    "\n",
    "metrics = ['accuracy_numeric', 'completeness_numeric', 'clarity_numeric', \n",
    "          'creativity_numeric', 'diversity_numeric', 'consistency_numeric']\n",
    "titles = ['Accuracy', 'Completeness', 'Clarity', 'Creativity', 'Diversity', 'Consistency']\n",
    "\n",
    "for i, (metric, title) in enumerate(zip(metrics, titles)):\n",
    "    row, col = i // 3, i % 3\n",
    "    \n",
    "    # Box plot for each temperature\n",
    "    df.boxplot(column=metric, by='temperature', ax=axes[row, col])\n",
    "    axes[row, col].set_title(f'{title} by Temperature')\n",
    "    axes[row, col].set_xlabel('Temperature')\n",
    "    axes[row, col].set_ylabel(f'{title} Score (1-5)')\n",
    "    axes[row, col].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temperature vs Performance Line Plot\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Calculate mean scores for each temperature\n",
    "temp_means = df.groupby('temperature')[metrics].mean()\n",
    "\n",
    "for i, (metric, title) in enumerate(zip(metrics, titles)):\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    plt.plot(temp_means.index, temp_means[metric], 'o-', linewidth=2, markersize=8)\n",
    "    plt.title(f'{title} vs Temperature')\n",
    "    plt.xlabel('Temperature')\n",
    "    plt.ylabel(f'{title} Score')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.ylim(1, 5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Analysis\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Create correlation matrix\n",
    "corr_cols = ['temperature', 'lexical_diversity', 'latency'] + metrics\n",
    "correlation_matrix = df[corr_cols].corr()\n",
    "\n",
    "# Plot heatmap\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "            square=True, fmt='.3f')\n",
    "plt.title('Temperature and Performance Metrics Correlation')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Temperature Analysis\n",
    "print(\"\\n=== TEMPERATURE COMPARISON ANALYSIS ===\")\n",
    "\n",
    "# Overall performance by temperature\n",
    "overall_performance = df.groupby('temperature')['overall_rating_numeric'].agg(['mean', 'std', 'count'])\n",
    "overall_performance.columns = ['Mean_Score', 'Std_Dev', 'Count']\n",
    "overall_performance = overall_performance.sort_values('Mean_Score', ascending=False)\n",
    "\n",
    "print(\"\\nOverall Performance Ranking:\")\n",
    "print(overall_performance)\n",
    "\n",
    "# Best temperature for each metric\n",
    "print(\"\\nBest Temperature for Each Metric:\")\n",
    "for metric, title in zip(metrics, titles):\n",
    "    best_temp = df.groupby('temperature')[metric].mean().idxmax()\n",
    "    best_score = df.groupby('temperature')[metric].mean().max()\n",
    "    print(f\"{title}: Temperature {best_temp} (Score: {best_score:.3f})\")\n",
    "\n",
    "# Temperature-specific insights\n",
    "print(\"\\nTemperature Insights:\")\n",
    "creativity_by_temp = df.groupby('temperature')['creativity_numeric'].mean()\n",
    "consistency_by_temp = df.groupby('temperature')['consistency_numeric'].mean()\n",
    "diversity_by_temp = df.groupby('temperature')['lexical_diversity'].mean()\n",
    "\n",
    "print(f\"Highest Creativity: Temperature {creativity_by_temp.idxmax()} ({creativity_by_temp.max():.3f})\")\n",
    "print(f\"Highest Consistency: Temperature {consistency_by_temp.idxmax()} ({consistency_by_temp.max():.3f})\")\n",
    "print(f\"Highest Lexical Diversity: Temperature {diversity_by_temp.idxmax()} ({diversity_by_temp.max():.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Recommendations\n",
    "print(\"\\n=== TEMPERATURE RECOMMENDATIONS ===\")\n",
    "\n",
    "best_overall = overall_performance.index[0]\n",
    "best_creativity = creativity_by_temp.idxmax()\n",
    "best_consistency = consistency_by_temp.idxmax()\n",
    "\n",
    "print(f\"\\nüèÜ BEST OVERALL: Temperature {best_overall}\")\n",
    "print(f\"   - Highest overall rating: {overall_performance.loc[best_overall, 'Mean_Score']:.3f}\")\n",
    "print(f\"   - Standard deviation: {overall_performance.loc[best_overall, 'Std_Dev']:.3f}\")\n",
    "\n",
    "print(f\"\\nüé® MOST CREATIVE: Temperature {best_creativity}\")\n",
    "print(f\"   - Creativity score: {creativity_by_temp[best_creativity]:.3f}\")\n",
    "\n",
    "print(f\"\\nüéØ MOST CONSISTENT: Temperature {best_consistency}\")\n",
    "print(f\"   - Consistency score: {consistency_by_temp[best_consistency]:.3f}\")\n",
    "\n",
    "print(\"\\nüìä USE CASE RECOMMENDATIONS:\")\n",
    "print(\"‚Ä¢ Factual Q&A: Use lower temperatures (0.0-0.5) for accuracy\")\n",
    "print(\"‚Ä¢ Creative content: Use higher temperatures (1.2-2.0) for diversity\")\n",
    "print(\"‚Ä¢ Balanced responses: Use medium temperatures (0.5-0.8)\")\n",
    "print(\"‚Ä¢ Production systems: Consider consistency vs creativity trade-offs\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
