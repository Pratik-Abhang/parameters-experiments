{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3417d7b3",
   "metadata": {},
   "source": [
    "## Top-k = limit the number of choices\n",
    "## Top-p = limit the total probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "368ce733",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "load_dotenv()\n",
    "client = OpenAI()\n",
    "\n",
    "# Test queries about Gemini\n",
    "TEST_QUERIES = [\n",
    "    \"What are the key capabilities of Gemini models?\",\n",
    "    \"How does Gemini compare to other multimodal models?\", \n",
    "    \"What are the different versions of Gemini?\",\n",
    "    \"What training data was used for Gemini?\",\n",
    "    \"What are the safety measures in Gemini models?\"\n",
    "]\n",
    "\n",
    "# Parameters to test\n",
    "TOP_P_VALUES = [0.2, 0.5, 0.9, 1.0]\n",
    "RETRIEVAL_K_VALUES = [1, 3, 5, 10, 15]\n",
    "\n",
    "TEMPERATURE = 0.7  # Keep constant\n",
    "MAX_TOKENS = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31c084cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 82 chunks for RAG experiments\n"
     ]
    }
   ],
   "source": [
    "# Load RAG embeddings\n",
    "with open('../data/rag_embeddings.pkl', 'rb') as f:\n",
    "    rag_data = pickle.load(f)\n",
    "\n",
    "chunks = rag_data['chunks']\n",
    "embeddings = rag_data['embeddings']\n",
    "\n",
    "print(f\"Loaded {len(chunks)} chunks for RAG experiments\")\n",
    "\n",
    "# RAG functions\n",
    "def get_embedding(text, model=\"text-embedding-3-small\"):\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    return client.embeddings.create(input=[text], model=model).data[0].embedding\n",
    "\n",
    "def retrieve_chunks(query, chunks, embeddings, k=5):\n",
    "    query_embedding = get_embedding(query)\n",
    "    similarities = cosine_similarity([query_embedding], embeddings)[0]\n",
    "    top_indices = np.argsort(similarities)[::-1][:k]\n",
    "    \n",
    "    retrieved_chunks = []\n",
    "    for idx in top_indices:\n",
    "        retrieved_chunks.append({\n",
    "            'chunk': chunks[idx],\n",
    "            'similarity': similarities[idx]\n",
    "        })\n",
    "    \n",
    "    return retrieved_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "802dedb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Judge function ready\n"
     ]
    }
   ],
   "source": [
    "# LLM Judge for RAG Answer Evaluation\n",
    "def llm_judge_rag(query, answer, context):\n",
    "    judge_prompt = f\"\"\"\n",
    "You are an expert evaluator. Assess this RAG answer based on the given context.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION: {query}\n",
    "\n",
    "ANSWER: {answer}\n",
    "\n",
    "Evaluate on these 3 criteria using this scale:\n",
    "- Poor: Incorrect, incomplete, or irrelevant\n",
    "- Below Average: Partially correct but missing key information  \n",
    "- Average: Correct but basic, meets minimum requirements\n",
    "- Good: Comprehensive, accurate, and well-structured\n",
    "- Excellent: Outstanding accuracy, completeness, and clarity\n",
    "\n",
    "CRITERIA:\n",
    "1. Accuracy: Is the answer factually correct based on the context?\n",
    "2. Completeness: Does it address all parts of the question?\n",
    "3. Clarity: Is it well-structured and easy to understand?\n",
    "\n",
    "Respond in this exact format:\n",
    "Accuracy: [Rating]\n",
    "Completeness: [Rating] \n",
    "Clarity: [Rating]\n",
    "Overall: [Rating]\n",
    "Reasoning: [Brief explanation]\n",
    "\"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": judge_prompt}],\n",
    "            temperature=0.1,\n",
    "            max_tokens=300\n",
    "        )\n",
    "        \n",
    "        evaluation = response.choices[0].message.content\n",
    "        lines = evaluation.strip().split('\\n')\n",
    "        result = {}\n",
    "        \n",
    "        for line in lines:\n",
    "            if ':' in line:\n",
    "                key, value = line.split(':', 1)\n",
    "                result[key.strip().lower()] = value.strip()\n",
    "        \n",
    "        return {\n",
    "            'accuracy': result.get('accuracy', 'N/A'),\n",
    "            'completeness': result.get('completeness', 'N/A'),\n",
    "            'clarity': result.get('clarity', 'N/A'),\n",
    "            'overall': result.get('overall', 'N/A'),\n",
    "            'reasoning': result.get('reasoning', 'N/A')\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'accuracy': 'Error',\n",
    "            'completeness': 'Error', \n",
    "            'clarity': 'Error',\n",
    "            'overall': 'Error',\n",
    "            'reasoning': f'Evaluation failed: {str(e)}'\n",
    "        }\n",
    "\n",
    "print(\"LLM Judge function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23184f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Top-p RAG experiments...\n",
      "Testing top_p=0.2 for query: 'What are the key capabilities of Gemini models?...'\n",
      "Testing top_p=0.5 for query: 'What are the key capabilities of Gemini models?...'\n",
      "Testing top_p=0.9 for query: 'What are the key capabilities of Gemini models?...'\n",
      "Testing top_p=1.0 for query: 'What are the key capabilities of Gemini models?...'\n",
      "Testing top_p=0.2 for query: 'How does Gemini compare to other multimodal models...'\n",
      "Testing top_p=0.5 for query: 'How does Gemini compare to other multimodal models...'\n",
      "Testing top_p=0.9 for query: 'How does Gemini compare to other multimodal models...'\n",
      "Testing top_p=1.0 for query: 'How does Gemini compare to other multimodal models...'\n",
      "\n",
      "Completed 8 Top-p experiments\n"
     ]
    }
   ],
   "source": [
    "# Experiment 1: Top-p Effect on RAG Responses\n",
    "def run_top_p_rag_experiment(query, top_p, retrieval_k=5):\n",
    "    start = time.time()\n",
    "    \n",
    "    # Retrieve chunks\n",
    "    retrieved = retrieve_chunks(query, chunks, embeddings, k=retrieval_k)\n",
    "    context = \"\\n\\n\".join([item['chunk']['text'] for item in retrieved])\n",
    "    \n",
    "    # Generate response with different top_p\n",
    "    prompt = f\"\"\"\n",
    "Use the context below to answer the question. Be accurate and cite specific information from the context.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=TEMPERATURE,\n",
    "        top_p=top_p,\n",
    "        max_tokens=MAX_TOKENS\n",
    "    )\n",
    "    \n",
    "    latency = time.time() - start\n",
    "    answer = response.choices[0].message.content\n",
    "    \n",
    "    # Get evaluation\n",
    "    evaluation = llm_judge_rag(query, answer, context)\n",
    "    \n",
    "    return {\n",
    "        \"experiment\": \"top_p\",\n",
    "        \"query\": query,\n",
    "        \"top_p\": top_p,\n",
    "        \"retrieval_k\": retrieval_k,\n",
    "        \"answer\": answer,\n",
    "        \"context_length\": len(context),\n",
    "        \"latency\": round(latency, 2),\n",
    "        \"tokens\": response.usage.total_tokens,\n",
    "        \"accuracy\": evaluation['accuracy'],\n",
    "        \"completeness\": evaluation['completeness'],\n",
    "        \"clarity\": evaluation['clarity'],\n",
    "        \"overall_rating\": evaluation['overall'],\n",
    "        \"reasoning\": evaluation['reasoning']\n",
    "    }\n",
    "\n",
    "# Run Top-p experiments\n",
    "print(\"Running Top-p RAG experiments...\")\n",
    "top_p_results = []\n",
    "\n",
    "for query in TEST_QUERIES[:2]:  # Test first 2 queries to save time\n",
    "    for top_p in TOP_P_VALUES:\n",
    "        print(f\"Testing top_p={top_p} for query: '{query[:50]}...'\")\n",
    "        result = run_top_p_rag_experiment(query, top_p, retrieval_k=5)\n",
    "        top_p_results.append(result)\n",
    "        time.sleep(1)\n",
    "\n",
    "top_p_df = pd.DataFrame(top_p_results)\n",
    "print(f\"\\nCompleted {len(top_p_results)} Top-p experiments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6340a1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experiment</th>\n",
       "      <th>query</th>\n",
       "      <th>top_p</th>\n",
       "      <th>retrieval_k</th>\n",
       "      <th>answer</th>\n",
       "      <th>context_length</th>\n",
       "      <th>latency</th>\n",
       "      <th>tokens</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>completeness</th>\n",
       "      <th>clarity</th>\n",
       "      <th>overall_rating</th>\n",
       "      <th>reasoning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>top_p</td>\n",
       "      <td>What are the key capabilities of Gemini models?</td>\n",
       "      <td>0.2</td>\n",
       "      <td>5</td>\n",
       "      <td>Gemini models exhibit several key capabilities...</td>\n",
       "      <td>17216</td>\n",
       "      <td>6.97</td>\n",
       "      <td>3818</td>\n",
       "      <td>Good</td>\n",
       "      <td>Good</td>\n",
       "      <td>Good</td>\n",
       "      <td>Good</td>\n",
       "      <td>The answer accurately reflects the key capabil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>top_p</td>\n",
       "      <td>What are the key capabilities of Gemini models?</td>\n",
       "      <td>0.5</td>\n",
       "      <td>5</td>\n",
       "      <td>Gemini models exhibit several key capabilities...</td>\n",
       "      <td>17216</td>\n",
       "      <td>5.33</td>\n",
       "      <td>3818</td>\n",
       "      <td>Good</td>\n",
       "      <td>Good</td>\n",
       "      <td>Good</td>\n",
       "      <td>Good</td>\n",
       "      <td>The answer accurately reflects the key capabil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>top_p</td>\n",
       "      <td>What are the key capabilities of Gemini models?</td>\n",
       "      <td>0.9</td>\n",
       "      <td>5</td>\n",
       "      <td>Gemini models exhibit several key capabilities...</td>\n",
       "      <td>17216</td>\n",
       "      <td>5.48</td>\n",
       "      <td>3818</td>\n",
       "      <td>Good</td>\n",
       "      <td>Average</td>\n",
       "      <td>Good</td>\n",
       "      <td>Good</td>\n",
       "      <td>The answer accurately describes the key capabi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>top_p</td>\n",
       "      <td>What are the key capabilities of Gemini models?</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>The key capabilities of Gemini models include:...</td>\n",
       "      <td>17216</td>\n",
       "      <td>5.64</td>\n",
       "      <td>3818</td>\n",
       "      <td>Good</td>\n",
       "      <td>Good</td>\n",
       "      <td>Good</td>\n",
       "      <td>Good</td>\n",
       "      <td>The answer accurately captures the key capabil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>top_p</td>\n",
       "      <td>How does Gemini compare to other multimodal mo...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>5</td>\n",
       "      <td>Gemini models, particularly the Gemini Ultra v...</td>\n",
       "      <td>16853</td>\n",
       "      <td>4.71</td>\n",
       "      <td>3737</td>\n",
       "      <td>Good</td>\n",
       "      <td>Good</td>\n",
       "      <td>Good</td>\n",
       "      <td>Good</td>\n",
       "      <td>The answer accurately reflects the context pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>top_p</td>\n",
       "      <td>How does Gemini compare to other multimodal mo...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>5</td>\n",
       "      <td>Gemini models, particularly the Gemini Ultra v...</td>\n",
       "      <td>16853</td>\n",
       "      <td>6.65</td>\n",
       "      <td>3737</td>\n",
       "      <td>Good</td>\n",
       "      <td>Good</td>\n",
       "      <td>Good</td>\n",
       "      <td>Good</td>\n",
       "      <td>The answer accurately reflects the advancement...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>top_p</td>\n",
       "      <td>How does Gemini compare to other multimodal mo...</td>\n",
       "      <td>0.9</td>\n",
       "      <td>5</td>\n",
       "      <td>The Gemini models, particularly the Gemini Ult...</td>\n",
       "      <td>16853</td>\n",
       "      <td>6.52</td>\n",
       "      <td>3737</td>\n",
       "      <td>Good</td>\n",
       "      <td>Good</td>\n",
       "      <td>Good</td>\n",
       "      <td>Good</td>\n",
       "      <td>The answer accurately reflects the advancement...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>top_p</td>\n",
       "      <td>How does Gemini compare to other multimodal mo...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>Gemini models, particularly the Gemini Ultra v...</td>\n",
       "      <td>16853</td>\n",
       "      <td>5.11</td>\n",
       "      <td>3737</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>Good</td>\n",
       "      <td>Good</td>\n",
       "      <td>Good</td>\n",
       "      <td>The answer accurately reflects the context pro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  experiment                                              query  top_p  \\\n",
       "0      top_p    What are the key capabilities of Gemini models?    0.2   \n",
       "1      top_p    What are the key capabilities of Gemini models?    0.5   \n",
       "2      top_p    What are the key capabilities of Gemini models?    0.9   \n",
       "3      top_p    What are the key capabilities of Gemini models?    1.0   \n",
       "4      top_p  How does Gemini compare to other multimodal mo...    0.2   \n",
       "5      top_p  How does Gemini compare to other multimodal mo...    0.5   \n",
       "6      top_p  How does Gemini compare to other multimodal mo...    0.9   \n",
       "7      top_p  How does Gemini compare to other multimodal mo...    1.0   \n",
       "\n",
       "   retrieval_k                                             answer  \\\n",
       "0            5  Gemini models exhibit several key capabilities...   \n",
       "1            5  Gemini models exhibit several key capabilities...   \n",
       "2            5  Gemini models exhibit several key capabilities...   \n",
       "3            5  The key capabilities of Gemini models include:...   \n",
       "4            5  Gemini models, particularly the Gemini Ultra v...   \n",
       "5            5  Gemini models, particularly the Gemini Ultra v...   \n",
       "6            5  The Gemini models, particularly the Gemini Ult...   \n",
       "7            5  Gemini models, particularly the Gemini Ultra v...   \n",
       "\n",
       "   context_length  latency  tokens   accuracy completeness clarity  \\\n",
       "0           17216     6.97    3818       Good         Good    Good   \n",
       "1           17216     5.33    3818       Good         Good    Good   \n",
       "2           17216     5.48    3818       Good      Average    Good   \n",
       "3           17216     5.64    3818       Good         Good    Good   \n",
       "4           16853     4.71    3737       Good         Good    Good   \n",
       "5           16853     6.65    3737       Good         Good    Good   \n",
       "6           16853     6.52    3737       Good         Good    Good   \n",
       "7           16853     5.11    3737  Excellent         Good    Good   \n",
       "\n",
       "  overall_rating                                          reasoning  \n",
       "0           Good  The answer accurately reflects the key capabil...  \n",
       "1           Good  The answer accurately reflects the key capabil...  \n",
       "2           Good  The answer accurately describes the key capabi...  \n",
       "3           Good  The answer accurately captures the key capabil...  \n",
       "4           Good  The answer accurately reflects the context pro...  \n",
       "5           Good  The answer accurately reflects the advancement...  \n",
       "6           Good  The answer accurately reflects the advancement...  \n",
       "7           Good  The answer accurately reflects the context pro...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_p_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f68f916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Retrieval-k experiments...\n",
      "Testing retrieval_k=1 for query: 'What are the key capabilities of Gemini models?...'\n",
      "Testing retrieval_k=3 for query: 'What are the key capabilities of Gemini models?...'\n",
      "Testing retrieval_k=5 for query: 'What are the key capabilities of Gemini models?...'\n",
      "Testing retrieval_k=10 for query: 'What are the key capabilities of Gemini models?...'\n",
      "Testing retrieval_k=15 for query: 'What are the key capabilities of Gemini models?...'\n",
      "Testing retrieval_k=1 for query: 'How does Gemini compare to other multimodal models...'\n",
      "Testing retrieval_k=3 for query: 'How does Gemini compare to other multimodal models...'\n",
      "Testing retrieval_k=5 for query: 'How does Gemini compare to other multimodal models...'\n",
      "Testing retrieval_k=10 for query: 'How does Gemini compare to other multimodal models...'\n",
      "Testing retrieval_k=15 for query: 'How does Gemini compare to other multimodal models...'\n",
      "\n",
      "Completed 10 Retrieval-k experiments\n"
     ]
    }
   ],
   "source": [
    "# Experiment 2: Retrieval-k Effect on RAG Responses  \n",
    "def run_retrieval_k_experiment(query, retrieval_k, top_p=0.9):\n",
    "    start = time.time()\n",
    "    \n",
    "    # Retrieve different numbers of chunks\n",
    "    retrieved = retrieve_chunks(query, chunks, embeddings, k=retrieval_k)\n",
    "    context = \"\\n\\n\".join([item['chunk']['text'] for item in retrieved])\n",
    "    \n",
    "    # Generate response\n",
    "    prompt = f\"\"\"\n",
    "Use the context below to answer the question. Be accurate and cite specific information from the context.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=TEMPERATURE,\n",
    "        top_p=top_p,\n",
    "        max_tokens=MAX_TOKENS\n",
    "    )\n",
    "    \n",
    "    latency = time.time() - start\n",
    "    answer = response.choices[0].message.content\n",
    "    \n",
    "    # Get evaluation\n",
    "    evaluation = llm_judge_rag(query, answer, context)\n",
    "    \n",
    "    return {\n",
    "        \"experiment\": \"retrieval_k\",\n",
    "        \"query\": query,\n",
    "        \"top_p\": top_p,\n",
    "        \"retrieval_k\": retrieval_k,\n",
    "        \"answer\": answer,\n",
    "        \"context_length\": len(context),\n",
    "        \"latency\": round(latency, 2),\n",
    "        \"tokens\": response.usage.total_tokens,\n",
    "        \"accuracy\": evaluation['accuracy'],\n",
    "        \"completeness\": evaluation['completeness'],\n",
    "        \"clarity\": evaluation['clarity'],\n",
    "        \"overall_rating\": evaluation['overall'],\n",
    "        \"reasoning\": evaluation['reasoning']\n",
    "    }\n",
    "\n",
    "# Run Retrieval-k experiments\n",
    "print(\"\\nRunning Retrieval-k experiments...\")\n",
    "retrieval_k_results = []\n",
    "\n",
    "for query in TEST_QUERIES[:2]:  # Test first 2 queries\n",
    "    for k in RETRIEVAL_K_VALUES:\n",
    "        print(f\"Testing retrieval_k={k} for query: '{query[:50]}...'\")\n",
    "        result = run_retrieval_k_experiment(query, k, top_p=0.9)\n",
    "        retrieval_k_results.append(result)\n",
    "        time.sleep(1)\n",
    "\n",
    "retrieval_k_df = pd.DataFrame(retrieval_k_results)\n",
    "print(f\"\\nCompleted {len(retrieval_k_results)} Retrieval-k experiments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653f2543",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64f4a5d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experiment</th>\n",
       "      <th>query</th>\n",
       "      <th>top_p</th>\n",
       "      <th>retrieval_k</th>\n",
       "      <th>answer</th>\n",
       "      <th>context_length</th>\n",
       "      <th>latency</th>\n",
       "      <th>tokens</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>completeness</th>\n",
       "      <th>clarity</th>\n",
       "      <th>overall_rating</th>\n",
       "      <th>reasoning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>retrieval_k</td>\n",
       "      <td>What are the key capabilities of Gemini models?</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1</td>\n",
       "      <td>The key capabilities of Gemini models, particu...</td>\n",
       "      <td>3393</td>\n",
       "      <td>7.66</td>\n",
       "      <td>877</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>Good</td>\n",
       "      <td>Good</td>\n",
       "      <td>Good</td>\n",
       "      <td>The answer accurately reflects the key capabil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>retrieval_k</td>\n",
       "      <td>What are the key capabilities of Gemini models?</td>\n",
       "      <td>0.9</td>\n",
       "      <td>3</td>\n",
       "      <td>The key capabilities of Gemini models include:...</td>\n",
       "      <td>10375</td>\n",
       "      <td>4.83</td>\n",
       "      <td>2528</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>Good</td>\n",
       "      <td>Good</td>\n",
       "      <td>Good</td>\n",
       "      <td>The answer accurately reflects the key capabil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>retrieval_k</td>\n",
       "      <td>What are the key capabilities of Gemini models?</td>\n",
       "      <td>0.9</td>\n",
       "      <td>5</td>\n",
       "      <td>The key capabilities of Gemini models include:...</td>\n",
       "      <td>17216</td>\n",
       "      <td>4.58</td>\n",
       "      <td>3818</td>\n",
       "      <td>Good</td>\n",
       "      <td>Good</td>\n",
       "      <td>Good</td>\n",
       "      <td>Good</td>\n",
       "      <td>The answer accurately reflects the key capabil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>retrieval_k</td>\n",
       "      <td>What are the key capabilities of Gemini models?</td>\n",
       "      <td>0.9</td>\n",
       "      <td>10</td>\n",
       "      <td>Gemini models exhibit several key capabilities...</td>\n",
       "      <td>33928</td>\n",
       "      <td>5.22</td>\n",
       "      <td>7204</td>\n",
       "      <td>Good</td>\n",
       "      <td>Average</td>\n",
       "      <td>Good</td>\n",
       "      <td>Good</td>\n",
       "      <td>The answer accurately describes key capabiliti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>retrieval_k</td>\n",
       "      <td>What are the key capabilities of Gemini models?</td>\n",
       "      <td>0.9</td>\n",
       "      <td>15</td>\n",
       "      <td>Gemini models exhibit several key capabilities...</td>\n",
       "      <td>50224</td>\n",
       "      <td>6.43</td>\n",
       "      <td>11217</td>\n",
       "      <td>Good</td>\n",
       "      <td>Good</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>Good</td>\n",
       "      <td>The answer accurately reflects the key capabil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>retrieval_k</td>\n",
       "      <td>How does Gemini compare to other multimodal mo...</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1</td>\n",
       "      <td>The Gemini models, particularly the Gemini Ult...</td>\n",
       "      <td>3449</td>\n",
       "      <td>4.43</td>\n",
       "      <td>925</td>\n",
       "      <td>Good</td>\n",
       "      <td>Below Average</td>\n",
       "      <td>Good</td>\n",
       "      <td>Average</td>\n",
       "      <td>The answer accurately describes the evaluation...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>retrieval_k</td>\n",
       "      <td>How does Gemini compare to other multimodal mo...</td>\n",
       "      <td>0.9</td>\n",
       "      <td>3</td>\n",
       "      <td>Gemini, specifically the Gemini Ultra model, d...</td>\n",
       "      <td>10168</td>\n",
       "      <td>4.48</td>\n",
       "      <td>2305</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>Good</td>\n",
       "      <td>Good</td>\n",
       "      <td>Good</td>\n",
       "      <td>The answer accurately reflects the capabilitie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>retrieval_k</td>\n",
       "      <td>How does Gemini compare to other multimodal mo...</td>\n",
       "      <td>0.9</td>\n",
       "      <td>5</td>\n",
       "      <td>Gemini models, particularly the Gemini Ultra v...</td>\n",
       "      <td>16853</td>\n",
       "      <td>4.29</td>\n",
       "      <td>3737</td>\n",
       "      <td>Good</td>\n",
       "      <td>Good</td>\n",
       "      <td>Good</td>\n",
       "      <td>Good</td>\n",
       "      <td>The answer accurately reflects the advancement...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>retrieval_k</td>\n",
       "      <td>How does Gemini compare to other multimodal mo...</td>\n",
       "      <td>0.9</td>\n",
       "      <td>10</td>\n",
       "      <td>Gemini, particularly its Ultra variant, demons...</td>\n",
       "      <td>33892</td>\n",
       "      <td>4.98</td>\n",
       "      <td>7784</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>Good</td>\n",
       "      <td>Good</td>\n",
       "      <td>Good</td>\n",
       "      <td>The answer accurately highlights Gemini Ultra'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>retrieval_k</td>\n",
       "      <td>How does Gemini compare to other multimodal mo...</td>\n",
       "      <td>0.9</td>\n",
       "      <td>15</td>\n",
       "      <td>Gemini models, particularly the Gemini Ultra v...</td>\n",
       "      <td>50561</td>\n",
       "      <td>5.18</td>\n",
       "      <td>11547</td>\n",
       "      <td>Good</td>\n",
       "      <td>Average</td>\n",
       "      <td>Good</td>\n",
       "      <td>Good</td>\n",
       "      <td>The answer accurately highlights Gemini Ultra'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    experiment                                              query  top_p  \\\n",
       "0  retrieval_k    What are the key capabilities of Gemini models?    0.9   \n",
       "1  retrieval_k    What are the key capabilities of Gemini models?    0.9   \n",
       "2  retrieval_k    What are the key capabilities of Gemini models?    0.9   \n",
       "3  retrieval_k    What are the key capabilities of Gemini models?    0.9   \n",
       "4  retrieval_k    What are the key capabilities of Gemini models?    0.9   \n",
       "5  retrieval_k  How does Gemini compare to other multimodal mo...    0.9   \n",
       "6  retrieval_k  How does Gemini compare to other multimodal mo...    0.9   \n",
       "7  retrieval_k  How does Gemini compare to other multimodal mo...    0.9   \n",
       "8  retrieval_k  How does Gemini compare to other multimodal mo...    0.9   \n",
       "9  retrieval_k  How does Gemini compare to other multimodal mo...    0.9   \n",
       "\n",
       "   retrieval_k                                             answer  \\\n",
       "0            1  The key capabilities of Gemini models, particu...   \n",
       "1            3  The key capabilities of Gemini models include:...   \n",
       "2            5  The key capabilities of Gemini models include:...   \n",
       "3           10  Gemini models exhibit several key capabilities...   \n",
       "4           15  Gemini models exhibit several key capabilities...   \n",
       "5            1  The Gemini models, particularly the Gemini Ult...   \n",
       "6            3  Gemini, specifically the Gemini Ultra model, d...   \n",
       "7            5  Gemini models, particularly the Gemini Ultra v...   \n",
       "8           10  Gemini, particularly its Ultra variant, demons...   \n",
       "9           15  Gemini models, particularly the Gemini Ultra v...   \n",
       "\n",
       "   context_length  latency  tokens   accuracy   completeness    clarity  \\\n",
       "0            3393     7.66     877  Excellent           Good       Good   \n",
       "1           10375     4.83    2528  Excellent           Good       Good   \n",
       "2           17216     4.58    3818       Good           Good       Good   \n",
       "3           33928     5.22    7204       Good        Average       Good   \n",
       "4           50224     6.43   11217       Good           Good  Excellent   \n",
       "5            3449     4.43     925       Good  Below Average       Good   \n",
       "6           10168     4.48    2305  Excellent           Good       Good   \n",
       "7           16853     4.29    3737       Good           Good       Good   \n",
       "8           33892     4.98    7784  Excellent           Good       Good   \n",
       "9           50561     5.18   11547       Good        Average       Good   \n",
       "\n",
       "  overall_rating                                          reasoning  \n",
       "0           Good  The answer accurately reflects the key capabil...  \n",
       "1           Good  The answer accurately reflects the key capabil...  \n",
       "2           Good  The answer accurately reflects the key capabil...  \n",
       "3           Good  The answer accurately describes key capabiliti...  \n",
       "4           Good  The answer accurately reflects the key capabil...  \n",
       "5        Average  The answer accurately describes the evaluation...  \n",
       "6           Good  The answer accurately reflects the capabilitie...  \n",
       "7           Good  The answer accurately reflects the advancement...  \n",
       "8           Good  The answer accurately highlights Gemini Ultra'...  \n",
       "9           Good  The answer accurately highlights Gemini Ultra'...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_k_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8a3440a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key Insights:\n",
    "# 1. Top-p affects response creativity vs consistency in RAG\n",
    "# 2. Retrieval-k affects information completeness vs noise\n",
    "# 3. More chunks (higher k) = more context but potential information overload\n",
    "# 4. Lower top-p = more conservative, consistent answers\n",
    "# 5. Higher top-p = more creative but potentially less accurate answers\n",
    "\n",
    "print(\"Notebook complete! Check the CSV files for detailed analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8309d12b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Completions.create() got an unexpected keyword argument 'top_k'. Did you mean 'top_p'?",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m results_top_k = []\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m top_k_values:\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     results_top_k.append(\u001b[43mrun_top_k_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m      8\u001b[39m df_top_k = pd.DataFrame(results_top_k)\n\u001b[32m      9\u001b[39m df_top_k[[\u001b[33m\"\u001b[39m\u001b[33mtop_k\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcompletion_tokens\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mlatency_sec\u001b[39m\u001b[33m\"\u001b[39m]]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mrun_top_k_experiment\u001b[39m\u001b[34m(top_k)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_top_k_experiment\u001b[39m(top_k):\n\u001b[32m      2\u001b[39m     start = time.time()\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgpt-4o-mini\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mPROMPT\u001b[49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTEMPERATURE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMAX_TOKENS\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m     latency = \u001b[38;5;28mround\u001b[39m(time.time() - start, \u001b[32m2\u001b[39m)\n\u001b[32m     13\u001b[39m     tokens = response.usage.completion_tokens\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\Desktop\\Strong Basics\\venv\\Lib\\site-packages\\openai\\_utils\\_utils.py:286\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    284\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    285\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: Completions.create() got an unexpected keyword argument 'top_k'. Did you mean 'top_p'?"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9580dee6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
